{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation and regression \n",
    "---\n",
    "__ correlation__\n",
    "---\n",
    "``Whether returns to different stock market indexes are related and, if so, in what way. ``\n",
    "\n",
    "  1. __``(correlation coefficient)``__ the direction and extent of linear association between two variables. \n",
    "  2. __``(scatter plot)``__  (+/-1) lie on a straight line with a positive (negative) slope, even if the slope of the line in the figure were different (but positive (negative)). \n",
    "  3. __``(0)``__  no __``linear``__ relation. \n",
    "  \n",
    "* __``Disadvantages``__: \n",
    "  * correlation measures the __``linear``__ association between two variables, but it may not always be reliable. Two variables can have a strong __``nolinear relation``__ and still have a very low correlation. For example X~N(0,1), Y=$X^2$\n",
    "  \n",
    "  * correlation also may be an unreliable measure when outliers are present in one or both of the series. __``How to deal with outliers?__\n",
    "  \n",
    "    * (step 1) determine whether a computed sample correlation changes greatly by removing a few outliers.\n",
    "    * (step 2) use __``judgement``__ to determine whether those outliers contain information about the two variables' relationship (and should thus be included in the correlation analysis) or contain no information (and should thus be excluded). \n",
    "  \n",
    " * correlation does not imply __``causation``__. Even if two variables are highly correlated, one does not necessarily cause the  other in the sense that certain values of one variables bring about the occurence of certain values of the other. \n",
    " \n",
    " * correlation can be spurious in the sense of misleadingly pointing towards associations between variables: \n",
    "    $$ high - age$$\n",
    "    $$ age - vocabulary$$\n",
    "    $$\\to high \\quad ?\\quad  vovabulary$$\n",
    "    \n",
    "   investment professionals must be cautious in basing investment strategies on high correlations. Spurious correlation may suggest investment strategies that appear profitable but actually would not be so, if implemented. \n",
    "    \n",
    "* __`` Significance of the Correlation Coefficient``__: assess whether apparent relationships between random variables are the result of chance. If we decide that the relationships do not result from chance, we will be inclined to use this information in predictions because a good prediction of one variable will help us predict the other variable. \n",
    "   * distribution of the underlying variables (normally distributed): \n",
    "     $$ t=\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}$$\n",
    "     where r: sample correlation; t: t-distribution with $n-2$ degrees of freedom.\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "__ Regression__\n",
    "---\n",
    "``Test hypothesis where X helps to explain Y (single).``\n",
    " \n",
    "\n",
    "* when the __``linear relationship``__ between the two variables is significant, linear regression provides a simple model for forecasting the value of one variable, known as the dependent variable, given the value of the second variables, known as the independent variable. \n",
    "   * regression analysis uses two principal types of data: \n",
    "      * cross-sectional \n",
    "      * time series. \n",
    "\n",
    "   Cross-sectional data involve many observations on $X$ and $Y$ for the same time period. Those observations could come from different companies, asset classes, investmnet funds, people, countries, r other entities, depending on the regression model.\n",
    "   \n",
    "   Time-series data use many observations from different time periods for the same company, asset class, investment fund, person, country, or other entiry, depending on the regression model. \n",
    "   \n",
    "   \n",
    "   * __``X explains Y``__: One estimate of $Y$ is $\\bar{Y}$, the average of $Y$. If a regression of $Y$ on $X$ tends to give more accurate estimats of $Y$ than $\\bar{Y}$, we say that the independent variable helps ``explain`` $Y$ because using that independent variable improves our estimates. \n",
    "   \n",
    "   \n",
    "   * Linear regression, also known as __``linear least squares``__, computes a line that best fits the observations.\n",
    "     * $\\frac{Cov(X, Y)}{Var(X)} \\to \\hat{b}_1$\n",
    "     * $(\\bar{X}, \\bar{Y})$ lies in the line $\\to \\hat{b}_0$.\n",
    "     \n",
    "     \n",
    "   * __``Assumptions``__: \n",
    "      1. linear in parameters $(b_0, b_1)$\n",
    "         * $Y=b_0+b_1X^2+\\epsilon$\n",
    "         * $Y=b_0 e^{b_1X}+\\epsilon$\n",
    "      2. $X$ is not random\n",
    "      3. $E[\\epsilon_t]=0$\n",
    "      4. $Var(\\epsilon_t)=\\sigma^2$\n",
    "      5. $E[\\epsilon_t\\epsilon_t]=0, t\\ne s$\n",
    "      6. $\\epsilon \\sim N(0,1)$\n",
    "      \n",
    "   * __``How well a given linear regression model captures the relationship between X and Y?``__\n",
    "      * A strong relation? \n",
    "      \n",
    "      $\\to$ reasonably certain that we could use this regression model to firecast $Y$ using $X$\n",
    "      \n",
    "      $\\to$ an inaccurate forecast. \n",
    "      \n",
    "      * __``The standard error of estimate (SEE)``__  measures the uncertainty:\n",
    "      $$\n",
    "      SEE=\\left(\\frac{\\sum_{i=1}^{n}\\left(\\hat{\\epsilon_i}\\right)^2}{n-2}\\right)^{1/2}\n",
    "      $$\n",
    "      \n",
    "      n-2: n observations- 2 parameters.\n",
    "      \n",
    "      * __``The Coefficient of Determination (R^2)``__ measures the fraction of the total variation in the dependent variable that is explained by the independent variable. \n",
    "      \n",
    "         *  __``(one independent variable)``__ $$R^2=r^2$$\n",
    "         * __``(general cases)``__ If we did not know the regression relationship, the best guess for the value of any particular observation of the dependent variable would simply be $\\bar{Y}$:\n",
    "         $$\n",
    "         \\sum_{i=1}^n\\frac{(Y_i-\\bar{Y})^2}{n-1}\n",
    "         $$\n",
    "    An alternative to using $\\bar{Y}$ to predict $Y$ is using the regression relationship to make that prediction:\n",
    "    $$ \\hat{Y}_i=\\hat{b}_0+\\hat{b}_1 X_i.\n",
    "    $$\n",
    " If the regression relationship works well, the error in predicting $Y$ using $X$ __``(unexplained variation)``__:\n",
    "         $$\n",
    "         \\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2.\n",
    "         $$\n",
    "should be much smaller than the error in predicting $Y$ using $\\bar{Y}$ __``(total variation)``__:\n",
    "         $$\n",
    "         \\sum_{i=1}^n(Y_i-\\bar{Y})^2.\n",
    "         $$\n",
    "         \n",
    "  $$\\to R^2=1-\\frac{Unexplained}{Total}$$\n",
    "\n",
    "      * __``Hypothesis Testing``__ \n",
    "$$ t=\\frac{\\hat{b}_1-b_1}{s_{\\hat{b}_1}}$$\n",
    "with n-2 degrees of freedom.\n",
    "\n",
    "      * __``Analysis of Varaince``__  \n",
    "$$\n",
    "F=\\frac{RSS/\\#slope_p}{SSE/(n-p )}=t^2 \\quad(p=1)\n",
    "$$\n",
    "      * __``Estimate Error``__  \n",
    "$$\n",
    "s_f^2=s^2\\left(1+\\frac{1}{n}+\\frac{(X-\\bar{X})^2}{(n-1)s_x^2}\\right)\n",
    "$$\n",
    "\n",
    "   * __``Limitations``__\n",
    "       * Regression relations can change over time \n",
    "       * The use of regression reuslts specific to investment contexts is that public knowledge of regression relationships may negate their future usefullness. \n",
    "       \n",
    "       \n",
    "       \n",
    "---\n",
    "`` Multiple ``\n",
    "* \n",
    "$$Y=0.50+0.75X_1$$\n",
    "$$Y=?+??X_1+???X_2$$\n",
    "    * we would generally find that $?? \\ne 0.75$ unless the second independent variable were uncorrelated with $X_1$\n",
    "\n",
    "    * if ??=0.6,can we say that for every 1-unit increase in X_1, we expect Y to increase by 0.6 units? No, we still expect Y to increase by 0.75 units when X_2 is not held constant. \n",
    "   * if $X_1=\\alpha+\\beta X_2+Z$, then $Y=?+0.6 Z$. \n",
    "   * 0.6 would represent the expected effect on $Y$ of a 1-unit increase in $X_1$ after removing the part of $X_1$ that is correlated with $X_2$. \n",
    "   \n",
    "* __``Assumptions``__\n",
    "    * ... +\n",
    "    * No exact linear relation exists between two or more of the independent variables. \n",
    "    \n",
    "* __``Notice!``__\n",
    "   * We should be confident that the assumptions of the regression model are met\n",
    "   * We should be cautious about predictions based on values of the independent variables that are outside the range of the data on which the model was estimated; such predictions are often unreliable. \n",
    "   * When predicting the dependent variable using a linear regression model, we encounter two types of uncertainty: \n",
    "      * uncertainty in the regression model itself\n",
    "      * uncertainty about the estimates of the regression model's parameters. For multiple regression, computing a prediction interval to properly incorporate both types of uncertainty requires matrix algebra. \n",
    "   * __``t_test``__: test the significance of coefficients individually\n",
    "   * __``F_test``__: test the significance of the regression as a whole. \n",
    "       * In a multiple regression, we cannot test the null hypothesis that all slope coefficients equal 0 based on t_tests that each individual slope coefficient equals 0, because the individual tests do not account for the effects of interactions among the independent variables. \n",
    "   * __``Adjusted R^2``__: If we add regression variables to the model, the amount of unexplained variation will decrease, and RSS will increase, if the new independent variable explains any of the unexplained variation in the model. Such a reduction occurs when the new independent variable is even __``slightly``__ correlated with the dependent variable and is not linear combination of other independent variables in the regression: \n",
    "     $$\n",
    "     \\bar{R}^2=1-\\left(\\frac{n-1}{n-k-1}\\right)(1-R^2)\n",
    "     $$\n",
    "\n",
    "* __``Violations``__\n",
    "    * __``Heteroskedasticity``__\n",
    "        * __``Consequences``__: Although heteroskedasticity does not affect the __``consistency``__ of the regression parameter estimator, it can lead to mistakes in inference [introduce bias into estimators of the standard error of regression coefficients] \n",
    "            * F_test is unreliable\n",
    "            * t_tests are unreliable\n",
    "            \n",
    "         When we ignore heteroskedasticity, we tend to find significant relationships where none actually exist. \n",
    "         \n",
    "         * __``Unconditional``__ $\\epsilon$ is not correlated with $X$ $\\to$ creates no major problems for statistical inference \n",
    "         \n",
    "         * __``Conditional``__ $\\to$ causes the most problems for statistical inference\n",
    "         \n",
    "        * __``Testing``__ \n",
    "             * ``Breusch and Pagan``: under the null hypothesis of no conditional heteroskedasticity,  nR^2 (from  the regression of the squared residuals on the independent variables from the original regression) will be a $\\chi^2$ random variable with the number of degrees of freedom equal to the number of independent variables in the regression. \n",
    "             \n",
    "        * __``Correcting``__\n",
    "           * __``robust standard errors``__: corrects the standard errors of the linear regression model's estimated coefficients to account for the conditional heteroskedasticity\n",
    "           * __``geenralized least squares``__: modifies the original equation in an attempt to eliminate the heteroskedasticity. \n",
    "                     \n",
    "    * __``Serial Correlation``__\n",
    "       * __``Consequences``__: As long as none of $X_i$ is a lagged value of $Y$, then the estimated parameters themselves will be __``consistent``__ and need not be adjusted for the effects of serial correlation; Otherwise, series correlation will cause all the parameter estimates to be __``inconsistent``__ and they will not be valid estimates of the true parameters. \n",
    "       \n",
    "            * __``Positive serial correlation``__: \n",
    "                 underestimate the population error variance. \n",
    "                 \n",
    "       * __``Testing``__\n",
    "          * __``Durbin and Watson``__: \n",
    "             $$\n",
    "             DW=\\frac{\\sum_{t=2}^{T}\\left(\\hat{\\epsilon}_t-\\hat{\\epsilon}_{t-1}\\right)^2}{\\sum_{t=1}^T\\hat{\\epsilon}_t^2}\n",
    "             $$\n",
    "             \n",
    "        * __``Correcting``__\n",
    "           * Adjust the coefficient standard errors \n",
    "               * ``Newey-West``\n",
    "           * Modify the regression equation\n",
    "           \n",
    " * __``Multicolinearity``__ With multicollinearity we can estimate the regression, but the interpretation of the regression output becomes problematic.  \n",
    "     * __``Consequence``__: Although the presence of multicollinearity does not affect the  __``consistency``__ of the OLS estimates of the regression coefficients, the estimates become extremely imprecise and unreliable. It becomes impossible to distinguish the individual impacts of the independent variables on the dependent variable. $\\sigma^2$ increase, t_tests decrease...\n",
    "    \n",
    "    * __``Testing``__ \n",
    "       * A high R^2 + low t_statistics       \n",
    "          \n",
    "    * __`` Correcting``__\n",
    "       * excluding one or more of the regression variables. \n",
    "       * often no solution based in theory\n",
    "           \n",
    "           \n",
    "           \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Model Specification & Errors in Specification\n",
    "---\n",
    "\n",
    "* __`` Misspecified Functional Form``__\n",
    "   * One or more important variables could be omitted from regression \n",
    "   * One or more of the regression variables may need to be transformed before estimating the regression \n",
    "   * The regression model pools data from different samples that should not be pooled. \n",
    "   \n",
    "   \n",
    "* __``Time-Series Misspecification (Independent Variables Correlated with Errors)``__: violate regression assumption 3 that the error term has mean 0, conditioned on the independent variables. \n",
    "   * If these assumption is violated, the estimated refression coefficients will be biased and inconsistent. \n",
    "   * __``Cases``__\n",
    "      * including lagged dependent variables as independent variable in regressions with serially correlated errors;\n",
    "      * including a function of a dependent variable as an independent variable, sometimes as a result of the incorrect dating of variables; and \n",
    "      * independent variables that are measured with error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Time-Series Analysis\n",
    "---\n",
    "\n",
    "Apply linear regression to a given time series. \n",
    "\n",
    "#### AR(1)\n",
    "$$ x_t=b_0+b_1x_{t-1}+\\epsilon_t$$\n",
    "\n",
    "__``Issues``__: the assumptions of the linear regression model are not satisfied. \n",
    "\n",
    "*  The residual errors are correlated instead of being uncorrelated $\\to$ causes estimates of $(b_0, b_1)$ to be __``inconsistent``__\n",
    "\n",
    "* The mean and/or variance of the time series changes over time $\\to$ regression results are __``invalid``__\n",
    "\n",
    "\n",
    " \n",
    "* __``Model Calibrate Issue``__: $X_{t-1}$ is a random variable! If we use OLS to estimate the model, our statistical inference may be __``invalid``__. To conduct valid statistical inference, we must make a key assumption in time-series analysis: __``We must assume that the time series we are modeling is covariance stationary``__\n",
    "   * $E[x_t]=\\mu, \\quad |\\mu|<\\infty$\n",
    "   * $Cov(x_t, x_{t-s})=\\lambda_s, \\quad |\\lambda_s|<\\infty$;\n",
    "\n",
    "  __``and that the errors are uncorrelated``__\n",
    "  \n",
    "__``Detecting Serially Correlated Errors in AR Model``__ DW statistic is invalid under this setting.\n",
    "* __``t_test``__ involving a residual autocorrelation and the standard error of the residual autocorrelation.\n",
    "* If a time series comes from an AR(1) model, then to be convariance stationary the absolute value of the lag coefficient must be less thatn 1.0\n",
    "   * __``Dickey-Fuller test``__\n",
    "\n",
    "\n",
    "__``Random Walk``__\n",
    "We can not use the regression methods we have discussed so far to estimate an AR(1) model on a time series that is actually a random walk. \n",
    "* Not __``mean reversion``__\n",
    "* Not a covariance-stationary time series\n",
    "\n",
    "\n",
    "#### MA(1)\n",
    "$$\n",
    "x_t=\\epsilon_t+\\theta \\epsilon_{t-1}\n",
    "$$\n",
    "* $\\to$ ARMA $\\to$ ARIMA \n",
    "* __``Issues``__: \n",
    "   * The parameters in ARMA models can be very unstable. \n",
    "      * Slight changes in the data sample or the initial guesses for the values of the ARMA parameters can result in very different final estimates of the ARMA parameters\n",
    "   * Choosing the right ARMA model is more of an art than a scicence\n",
    "      * The criteria for deciding on p and q are far from perfect\n",
    "   * Even after a model is selected, that model may not forecast well. \n",
    "   \n",
    "   \n",
    "\n",
    "#### Other Issues \n",
    "* __``Seasonality``__\n",
    "  * add a seasonality lag \n",
    "  \n",
    "* __``AR Conditional Heteroskedasticity (ARCH)``__ The variance of the error in a particular time-series model in one period depends on the variance of the error in previous periods. \n",
    "   * ARCH(1):\n",
    "     $$\n",
    "     \\epsilon_t\\sim N(0, a_0+a_1\\epsilon_{t-1}^2)\n",
    "     $$\n",
    "     \n",
    "   * __``Engle``__ shows that we can test whether a time series is ARCH(1) by regressing the squared residuals from a previously estimated time-series model (AR, MA or ARMA) on a constant and one lag of the squared residuals. For example, \n",
    "      $$\n",
    "      \\hat{\\epsilon}_t^2=a_0+a_1\\hat{\\epsilon}_{t-1}^2+u_t\n",
    "      $$\n",
    "      \n",
    "   * __``consequence``__: the standard errors for the regression parameters will not be correct. We will need to use __``generalized least squares``__ or other methods that correct for heteroskedasticity to correctly estimate the standard error of the parameters in the time-series model. \n",
    "   \n",
    "* __``Regressions with more than one time series``__ A time series that contains a unit root is not covariance stationary. If any time series in a linear regression contains a unit root, OLS estimates of regression test statistics may be invalid.\n",
    "  * __``Duckey-Fuller test``__ \n",
    "     * Neither X or Y has a unit root $\\to$ OK!\n",
    "     * Either X or Y has a unit root but not both $\\to$ not covariance stationary $\\to$ __``inconsistent results``__\n",
    "     * Both X and Y have a unit root $\\to$ test for __``cointegration``__\n",
    "        * No $\\to$ __``inconsistent results``__\n",
    "        * Yes $\\to$ OK!\n",
    "     \n",
    "   * __``Coitegrated:``__ a __``long-term``__ financial or economic relationship exists between them such that they do not diverge from each other without bound in the long run. \n",
    "     * __``Cautious``__ in interpreting the results of a regression with cointegrated variables. The cointegrated regression estimates the __``long-term``__ relation between the two series but may not be the best model of the __``short-term``__ relation between the two series. \n",
    "     * __``Testing``__\n",
    "        * Estimate the regression $y_t=b_0+b_1x_t+\\epsilon_t$\n",
    "        * test whether the error term from the regression has a unit root using __``(Engle-Granger) Dickey-Fuller test``__\n",
    "        * if test fails $\\to$ not cointegrated \n",
    "        * if test successes $\\to$ cointegrated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
